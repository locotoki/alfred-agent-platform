name: ML Trainer Benchmark

on:
  push:
    branches:
      - 'feat/ml-retrain-pipeline'
      - 'feat/alert-dataset-db'
      - 'feat/dynamic-threshold-opt'
      - 'feat/hf-transformers-integr'
      - 'feat/phase8.3-sprint4'
  pull_request:
    paths:
      - 'alfred/ml/**'
      - 'alfred/alerts/**'
      - 'backend/alfred/ml/**'
      - 'backend/alfred/alerts/**'
      - '.github/workflows/trainer-benchmark.yml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 3  # Must complete in 3 minutes

    # Skip this job for SC-320, will be fixed in #220
    if: ${{ !contains(github.head_ref, 'sc-320') }}

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: alerts
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    env:
      ALERT_DB_URI: postgresql://test:test@localhost:5432/alerts
      PYTHONPATH: ${{ github.workspace }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Cache ML models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface
          key: ${{ runner.os }}-hf-models-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-hf-models-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install sentence-transformers psutil ray[default]

      - name: Initialize test database
        env:
          PGPASSWORD: test
        run: |
          echo "Creating test alert data..."
          python -c "
          import os
          import json
          from sqlalchemy import create_engine, text
          from datetime import datetime, timedelta
          import random

          engine = create_engine(os.environ['ALERT_DB_URI'])

          # Create alerts table if not exists
          with engine.connect() as conn:
              conn.execute(text('''
                  CREATE TABLE IF NOT EXISTS alerts (
                      id SERIAL PRIMARY KEY,
                      message TEXT NOT NULL,
                      source VARCHAR(100),
                      severity VARCHAR(20),
                      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                      metadata JSONB
                  )
              '''))
              conn.commit()

              # Insert test data
              alerts = []
              for i in range(1000):
                  severity = random.choice(['critical', 'warning', 'info'])
                  message = f'Test alert {i}: {severity} level event'
                  metadata = {'host': f'server-{i % 10}', 'app': f'app-{i % 5}'}
                  alerts.append({
                      'message': message,
                      'source': 'test',
                      'severity': severity,
                      'created_at': datetime.utcnow() - timedelta(hours=i),
                      'metadata': json.dumps(metadata)
                  })

              conn.execute(text('''
                  INSERT INTO alerts (message, source, severity, created_at, metadata)
                  VALUES (:message, :source, :severity, :created_at, :metadata::jsonb)
              '''), alerts)
              conn.commit()

          print('Test database initialized with 1000 alerts')
          "

      - name: Run HF transformer benchmark
        run: |
          echo "Running HuggingFace transformer benchmark..."
          python -c "
          import time
          import json
          import psutil
          import os
          from alfred.ml import HFEmbedder

          # Initialize embedder
          embedder = HFEmbedder()
          embedder.warmup()

          # Test texts
          test_texts = [
              'Critical database connection error on server-1',
              'Warning: High memory usage detected on app-2',
              'Info: Daily backup completed successfully',
              'Critical authentication failure from IP 192.168.1.100',
              'Warning: Disk space low on /var/log partition'
          ] * 10  # 50 texts total

          # Benchmark embedding
          start_time = time.time()
          embeddings = embedder.embed(test_texts)
          total_time = time.time() - start_time

          # Calculate per-text latency
          per_text_latency = (total_time / len(test_texts)) * 1000  # ms

          # Memory usage
          process = psutil.Process()
          memory_usage = process.memory_info().rss / 1024 / 1024  # MB

          # Test similarity calculation
          start_time = time.time()
          for i in range(10):
              similarity = embedder.cosine_similarity(embeddings[0], embeddings[i])
          similarity_time = (time.time() - start_time) / 10 * 1000  # ms

          results = {
              'total_texts': len(test_texts),
              'total_time': total_time,
              'per_text_latency_ms': per_text_latency,
              'similarity_latency_ms': similarity_time,
              'memory_usage_mb': memory_usage,
              'embedding_dimension': embeddings.shape[1]
          }

          with open('hf_benchmark_results.json', 'w') as f:
              json.dump(results, f, indent=2)

          print(f'Total texts: {len(test_texts)}')
          print(f'Total time: {total_time:.2f}s')
          print(f'Per-text latency: {per_text_latency:.2f}ms')
          print(f'Similarity latency: {similarity_time:.2f}ms')
          print(f'Memory usage: {memory_usage:.0f}MB')
          print(f'Embedding dimension: {embeddings.shape[1]}')
          "

      - name: Check performance targets
        run: |
          python -c "
          import json

          with open('hf_benchmark_results.json') as f:
              data = json.load(f)

          # Check performance targets
          latency = data['per_text_latency_ms']
          similarity_latency = data['similarity_latency_ms']
          memory = data['memory_usage_mb']

          print(f'Per-text inference: {latency:.2f}ms')
          print(f'Similarity calculation: {similarity_latency:.2f}ms')
          print(f'Memory usage: {memory:.0f}MB')

          # Enforce targets
          assert latency < 15, f'Inference latency {latency:.1f}ms exceeds 15ms target'
          assert similarity_latency < 1, f'Similarity calc {similarity_latency:.1f}ms exceeds 1ms target'
          assert memory < 1000, f'Memory usage {memory:.0f}MB exceeds 1GB target'

          print('✅ All performance targets met!')
          "

      - name: Run MLflow integration test
        run: |
          echo "Testing MLflow model registry integration..."
          python -c "
          import json
          import mlflow
          import mlflow.sklearn
          from sklearn.ensemble import RandomForestClassifier
          import numpy as np

          # Mock MLflow server (in CI, would use actual server)
          mlflow.set_tracking_uri('sqlite:///mlflow.db')
          mlflow.set_experiment('ci-benchmark')

          # Create a simple model
          X = np.random.rand(100, 10)
          y = np.random.randint(0, 2, 100)
          model = RandomForestClassifier(n_estimators=10)
          model.fit(X, y)

          # Log model to MLflow
          with mlflow.start_run() as run:
              mlflow.sklearn.log_model(
                  model,
                  'model',
                  registered_model_name='alert-noise-ranker-ci'
              )

              # Log benchmark results
              with open('hf_benchmark_results.json') as f:
                  metrics = json.load(f)

              mlflow.log_metrics({
                  'inference_latency_ms': metrics['per_text_latency_ms'],
                  'similarity_latency_ms': metrics['similarity_latency_ms'],
                  'memory_usage_mb': metrics['memory_usage_mb']
              })

              # Log model URI
              model_uri = f'runs:/{run.info.run_id}/model'
              mlflow.log_param('model_uri', model_uri)

              print(f'Model logged with URI: {model_uri}')
              print(f'Run ID: {run.info.run_id}')

              # Save model URI for artifact
              with open('model_uri.txt', 'w') as f:
                  f.write(model_uri)
          "

      - name: Upload benchmark report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: trainer-benchmark-report
          path: |
            hf_benchmark_results.json
            model_uri.txt

      - name: Post results to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const data = JSON.parse(fs.readFileSync('hf_benchmark_results.json'));
            let modelUri = '';

            try {
              modelUri = fs.readFileSync('model_uri.txt', 'utf8').trim();
            } catch(e) {
              modelUri = 'Not available';
            }

            const comment = `## 🎯 ML Trainer Benchmark Results

            ✅ Completed in < 3 minutes

            **HuggingFace Transformer Metrics:**
            - 📦 Total Texts: ${data.total_texts}
            - ⚡ Per-text Inference: ${data.per_text_latency_ms.toFixed(2)}ms (target < 15ms)
            - 🔗 Similarity Calculation: ${data.similarity_latency_ms.toFixed(2)}ms
            - 🧠 Memory Usage: ${data.memory_usage_mb.toFixed(0)}MB (target < 1GB)
            - 📏 Embedding Dimension: ${data.embedding_dimension}

            **MLflow Integration:**
            - ✅ Model logged to MLflow registry
            - 📦 Model URI: \`${modelUri}\`
            - 🏷️ Registered as: alert-noise-ranker-ci

            **Model Caching:**
            - ✅ HuggingFace models cached via GitHub Actions
            - ✅ Cache key: ${process.env.RUNNER_OS}-hf-models-*

            [Download Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
