name: Mock Performance Gate

on:
  pull_request:
    paths:
      - 'internal/handler/**'
      - 'cmd/server/**'
      - 'perf/**'
  workflow_dispatch:

env:
  GO_ENABLED: 0  # Set to 1 when Go environment is available

jobs:
  perf-gate:
    name: Performance Gate Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Check if real harness can run
        id: check-env
        run: |
          if [ "${{ env.GO_ENABLED }}" = "1" ] && command -v go >/dev/null 2>&1; then
            echo "mode=real" >> $GITHUB_OUTPUT
          else
            echo "mode=mock" >> $GITHUB_OUTPUT
          fi

      - name: Run mock performance test
        if: steps.check-env.outputs.mode == 'mock'
        run: |
          echo "‚ö†Ô∏è Running mock performance test (GO_ENABLED=0)"
          python perf/mock_performance_results.py | tee perf_results.txt

          # Extract metrics
          P95=$(grep -oE 'p95_latency_ms: ([0-9.]+)' perf_results.txt | awk '{print $2}')
          ERR=$(grep -oE 'error_rate: ([0-9.]+)' perf_results.txt | awk '{print $2}')

          echo "üìä Performance Results:"
          echo "  p95 latency: ${P95}ms"
          echo "  error rate: ${ERR}%"

          # Check pass criteria
          if (( $(echo "$P95 < 300" | bc -l) )) && (( $(echo "$ERR < 1" | bc -l) )); then
            echo "‚úÖ Performance gate PASSED"
          else
            echo "‚ùå Performance gate FAILED"
            exit 1
          fi

      - name: Run real performance test
        if: steps.check-env.outputs.mode == 'real'
        run: |
          echo "üöÄ Running real performance test"
          # Build and start services
          go build -o alfred ./cmd/alfred/main.go
          ./alfred up -d pg redis minio server
          sleep 20

          # Run harness
          export TARGET_URL=http://localhost:8080/v1/query
          export QPS=10
          export DURATION=60
          python perf/harness_scaffold.py | tee perf_results.txt

          # Extract and check metrics (same as mock)
          P95=$(grep -oE 'p95.*([0-9.]+)ms' perf_results.txt | grep -oE '[0-9.]+' | tail -1)
          ERR=$(grep -oE 'error_rate.*([0-9.]+)' perf_results.txt | grep -oE '[0-9.]+' | tail -1)

          echo "üìä Performance Results:"
          echo "  p95 latency: ${P95}ms"
          echo "  error rate: ${ERR}%"

          if [ "$P95" -lt "300" ] && [ "$ERR" -lt "1" ]; then
            echo "‚úÖ Performance gate PASSED"
          else
            echo "‚ùå Performance gate FAILED"
            exit 1
          fi

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: perf_results.txt
